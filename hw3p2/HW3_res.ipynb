{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykh8vQc0p71n"
      },
      "source": [
        "# 11785 HW3P2: Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0m4w_KkMeQ"
      },
      "source": [
        "Welcome to HW3P2. In this homework, you will be using the same data from HW1 but will be incorporating sequence models. We recommend you get familaried with sequential data and the working of RNNs, LSTMs and GRUs to have a smooth learning in this part of the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEll1kGkhcR"
      },
      "source": [
        "Disclaimer: This starter notebook will not be as elaborate as that of HW1P2 or HW2P2. You will need to do most of the implementation in this notebook because, it is expected after 2 HWs, you will be in a position to write a notebook from scratch. You are welcomed to reuse the code from the previous starter notebooks but may also need to make appropriate changes for this homework. <br>\n",
        "We have also given you 3 log files for the Very Low Cutoff (Levenshtein Distance = 30) so that you can observe how loss decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHGaJ_8tx_5Z"
      },
      "source": [
        "Common errors which you may face\n",
        "\n",
        "\n",
        "*   Shape errors: Half of the errors from this homework will account to this category. Try printing the shapes between intermediate steps to debug\n",
        "*   CUDA out of Memory: When your architecture has a lot of parameters, this can happen. Golden keys for this is, (1) Reducing batch_size (2) Call *torch.cuda.empty_cache* often, even inside your training loop, (3) Call *gc.collect* if it helps and (4) Restart run time if nothing works\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fwJWcpqJDR"
      },
      "source": [
        "# Prelimilaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"linjiw\",\"key\":\"e62f97a62e3404bfbd45c4b33990d364\"}') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c 11-785-s22-hw3p2\n",
        "! unzip 11-785-s22-hw3p2.zip"
      ],
      "metadata": {
        "id": "c7gxNVo80Kd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XkC4TOp0fyg",
        "outputId": "7094eb99-19ce-43b5-d32b-5dbdd14b530f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "# f28f905cf0d1b2c32ca1a1e437fb871c2b0e14c2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBSuC5Ua0hfF",
        "outputId": "5c0502ac-3e64-46b0-e740-f5265c2efa8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.8-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=59b77951413f75cd9f4934516cb85eaa5f3b473f0602ef4df730363307e2237e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.8 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.11 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leymyQ-apwT6"
      },
      "source": [
        "You will need to install packages for decoding and calculating the Levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZCQtZtkaTrcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e692b971-14ba-4bea-aa35-127ae2e48261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149866 sha256=9c1ac76c107789ceb32fb19635eee32b6a8f021f4a679c08fae28ef4094b468c\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n",
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 28 (delta 13), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 780.91 KiB | 11.83 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14073, done.        \n",
            "remote: Counting objects: 100% (386/386), done.        \n",
            "remote: Compressing objects: 100% (312/312), done.        \n",
            "remote: Total 14073 (delta 117), reused 134 (delta 60), pack-reused 13687        \n",
            "Receiving objects: 100% (14073/14073), 5.82 MiB | 10.24 MiB/s, done.\n",
            "Resolving deltas: 100% (7997/7997), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=ec3cbb8da950af3f4fbae13e8d7da3ef135ea0f8955f1dec895229b4073c88d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "/content/ctcdecode\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13234221 sha256=4602b9c1029f12e4a1f58f27e6673f962e916bb9e71525f7a60d7637b1c0c464\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-otue3ga1/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n",
            "/content\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (3.10.0.2)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX # We also install a summary package to check our model's forward before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "7f0506c2-ae52-4bb0-aa12-c89fca29fdbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinjiw\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220328_140329-182ferbj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/linjiw/HW3/runs/182ferbj\" target=\"_blank\">glad-durian-34</a></strong> to <a href=\"https://wandb.ai/linjiw/HW3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from os.path import join\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import phonemes\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "# from tqdm import tqdm_notebook as tqdm\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"HW3\", entity=\"linjiw\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "# !jupyter nbextension enable --py widgetsnbextension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6gTI0Rslxrr"
      },
      "source": [
        "You need to set up your Kaggle and download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPBUd7Cnl-Rx",
        "outputId": "69b4f22d-8402-4eb3-92a4-dcb0a0a2861e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s22-hw3p2.zip to /home/linjiw/Downloads/hw3/HW3 Starter\n",
            "100%|█████████████████████████████████████▉| 1.84G/1.84G [01:49<00:00, 15.5MB/s]\n",
            "100%|██████████████████████████████████████| 1.84G/1.84G [01:49<00:00, 18.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ! kaggle competitions download -c 11-785-s22-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if2Somqfbje1"
      },
      "outputs": [],
      "source": [
        "# ! unzip 11-785-s22-hw3p2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUCKqm1ST1sU"
      },
      "source": [
        "# Dataset and dataloading (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-PjVxPCBvVR6"
      },
      "outputs": [],
      "source": [
        "# PHONEME_MAP is the list that maps the phoneme to a single character. \n",
        "# The dataset contains a list of phonemes but you need to map them to their corresponding characters to calculate the Levenshtein Distance\n",
        "# You final submission should not have the phonemes but the mapped string\n",
        "# No TODOs in this cell\n",
        "\n",
        "PHONEME_MAP = [\n",
        "    \" \",\n",
        "    \".\", #SIL\n",
        "    \"a\", #AA\n",
        "    \"A\", #AE\n",
        "    \"h\", #AH\n",
        "    \"o\", #AO\n",
        "    \"w\", #AW\n",
        "    \"y\", #AY\n",
        "    \"b\", #B\n",
        "    \"c\", #CH\n",
        "    \"d\", #D\n",
        "    \"D\", #DH\n",
        "    \"e\", #EH\n",
        "    \"r\", #ER\n",
        "    \"E\", #EY\n",
        "    \"f\", #F\n",
        "    \"g\", #G\n",
        "    \"H\", #H\n",
        "    \"i\", #IH \n",
        "    \"I\", #IY\n",
        "    \"j\", #JH\n",
        "    \"k\", #K\n",
        "    \"l\", #L\n",
        "    \"m\", #M\n",
        "    \"n\", #N\n",
        "    \"N\", #NG\n",
        "    \"O\", #OW\n",
        "    \"Y\", #OY\n",
        "    \"p\", #P \n",
        "    \"R\", #R\n",
        "    \"s\", #S\n",
        "    \"S\", #SH\n",
        "    \"t\", #T\n",
        "    \"T\", #TH\n",
        "    \"u\", #UH\n",
        "    \"U\", #UW\n",
        "    \"v\", #V\n",
        "    \"W\", #W\n",
        "    \"?\", #Y\n",
        "    \"z\", #Z\n",
        "    \"Z\" #ZH\n",
        "]\n",
        "phe_dict = {}\n",
        "tensor_dict = {}\n",
        "PHONEMES = phonemes.PHONEMES\n",
        "for idx, i in enumerate(PHONEMES):\n",
        "    phe_dict[i] = PHONEME_MAP[idx]\n",
        "    tensor_dict[PHONEME_MAP[idx]] = idx\n",
        "\n",
        "def maplst(lst):\n",
        "    res =[]\n",
        "    for i in lst:\n",
        "        res.append(phe_dict[i])\n",
        "    res = np.array(res)\n",
        "    # res = res.astype(np.float)\n",
        "    return np.array(res)\n",
        "def maptotensor(lst):\n",
        "    res =[]\n",
        "    for i in lst:\n",
        "        res.append(tensor_dict[i])\n",
        "    res = np.array(res)\n",
        "    # res = res.astype(np.float)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q6M32260EDP",
        "outputId": "7a4f618f-aa0a-4ccd-cde8-f996ff21713e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['b' 'i' 'k' 'S' 'a']\n",
            "[ 8 18 21 31  2]\n"
          ]
        }
      ],
      "source": [
        "lst = ['B', 'IH', 'K', 'SH', 'AA']\n",
        "res = maplst(lst)\n",
        "tsr = maptotensor(res)\n",
        "print(res)\n",
        "print(tsr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rKIOl9z0EDQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0AbNHfyx0EDQ"
      },
      "outputs": [],
      "source": [
        "# This cell is where your actual TODOs start\n",
        "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
        "# The steps for implementation given below are how we have implemented it.\n",
        "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_dir = os.path.join(data_path,partition,\"mfcc/\")# TODO: get mfcc directory path\n",
        "        self.Y_dir = os.path.join(data_path,partition,\"transcript/\")# TODO: get transcript path\n",
        "\n",
        "        self.X_files = os.listdir(self.X_dir)# TODO: list files in the mfcc directory\n",
        "        self.Y_files = os.listdir(self.Y_dir)# TODO: list files in the transcript directory\n",
        "\n",
        "        # TODO: store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
        "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
        "        self.PHONEMES = phonemes.PHONEMES\n",
        "\n",
        "        assert(len(self.X_files) == len(self.Y_files))\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        X_path = self.X_dir + self.X_files[ind]\n",
        "        Y_path = self.Y_dir + self.Y_files[ind]\n",
        "        X = torch.Tensor(np.load(X_path))# TODO: Load the mfcc npy file at the specified index ind in the directory\n",
        "        # Y = maplst(np.load(Y_path)[1:-1])# TODO: Load the corresponding transcripts\n",
        "        Y = np.load(Y_path)[1:-1]\n",
        "        # print(Y)\n",
        "        # print(Y.shape)\n",
        "        # Y2 = PHONEMES.index(i) for i in np.load(Y_path)[1:-1]\n",
        "        # print(f\"Y {Y}\")\n",
        "        # print(f\"Y2 {Y2}\")\n",
        "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
        "        # You need to convert these into a sequence of Long tensors\n",
        "        # Tip: You may need to use self.PHONEMES\n",
        "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them. \n",
        "        # You need to remove '<sos>' and '<eos>' from the trancripts. \n",
        "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
        "        Yy = torch.LongTensor([PHONEMES.index(i) for i in Y])\n",
        "        # Yy = torch.Tensor(maptotensor(Y)).type(torch.LongTensor)# TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "        # print(f\"X {X.shape}\")\n",
        "        # print(f\"Y {Y.shape}\")\n",
        "        return X, Yy\n",
        "    \n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "        # print(batch_x[0].shape)\n",
        "\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_x):\n",
        "            new_lst.append(batch_x[idx])\n",
        "        batch_x_pad = pad_sequence(new_lst)\n",
        "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "        lengths_x_pad = [len(i) for i in batch_x_pad]\n",
        "        # print(f\"batch_x {batch_x}\")\n",
        "        \n",
        "\n",
        "        # print(f\"test_pad.len {test_pad.shape}\")\n",
        "        # print(f\"batch_x.len {len(batch_x)}\")\n",
        "        # print(f\"lengths_x {lengths_x}\")\n",
        "        # print(f\"lengths_x_pad {lengths_x_pad}\")\n",
        "\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_y):\n",
        "            new_lst.append(batch_y[idx])\n",
        "        batch_y_pad = pad_sequence(new_lst)\n",
        "\n",
        "        # batch_y_pad = pad_sequence(batch_y) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [len(i) for i in batch_y] # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M2mvCvY0EDR"
      },
      "outputs": [],
      "source": [
        "root = 'hw3p2_student_data/hw3p2_student_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCoiXHP70EDR",
        "outputId": "e9142a1e-aaa1-43e6-c918-2418daad25bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X torch.Size([1428, 13])\n",
            "Y (130,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.2427e+01, -1.1178e+00, -2.4150e-01,  ..., -2.0521e-03,\n",
              "          -1.5512e-01, -1.6788e-01],\n",
              "         [ 7.2623e+00, -1.0281e-01, -1.1216e+00,  ..., -1.3412e-01,\n",
              "          -9.4587e-02,  1.3803e-01],\n",
              "         [ 6.7271e+00, -5.3644e-02, -8.4255e-01,  ...,  1.2032e-02,\n",
              "          -9.2675e-02, -8.1234e-02],\n",
              "         ...,\n",
              "         [ 5.5238e+00, -6.6449e-01, -2.4509e-01,  ...,  7.9069e-02,\n",
              "           7.8216e-02,  1.0809e-01],\n",
              "         [ 5.5423e+00, -4.0494e-01, -2.4527e-01,  ..., -2.4723e-02,\n",
              "          -1.0742e-01, -7.8955e-02],\n",
              "         [ 5.8132e+00, -4.8034e-01, -4.6667e-01,  ..., -6.9344e-02,\n",
              "          -5.6162e-02,  9.8189e-02]]),\n",
              " tensor([ 1., 37., 18.,  9.,  1., 12., 24., 10., 18., 10., 18., 24.,  9.,  3.,\n",
              "         24., 32., 19., 39.,  1.,  4., 32., 13., 10., 18., 15., 19., 32.,  1.,\n",
              "         15.,  5., 29., 17., 19., 10.,  7., 10., 15., 13., 23., 17., 18., 39.,\n",
              "         37., 35., 24., 10., 39.,  1., 10.,  6., 24., 19.,  3., 24., 10., 30.,\n",
              "         24., 26.,  8.,  5., 22., 30., 35., 24., 15.,  2., 22., 26., 10.,  1.,\n",
              "         15.,  5., 29., 11.,  4., 32., 35., 30., 37., 19., 32.,  1., 22., 18.,\n",
              "         32.,  4., 22., 33., 18., 25., 39., 37., 34., 10., 30., 37., 18., 25.,\n",
              "          2., 24., 11.,  4.,  8.,  4., 29., 10.,  4., 21., 22., 19., 36., 39.,\n",
              "          1., 11.,  4., 32., 16., 29., 35., 26., 36., 13., 11.,  4.,  8., 29.,\n",
              "         34., 21.,  1.,  1.]))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lsmp = LibriSamples(root,\"train\")\n",
        "lsmp.__getitem__(1)\n",
        "# lsmp.collate_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, test_order): # test_order is the csv similar to what you used in hw1\n",
        "        self.data_path = data_path\n",
        "        test_csv_pth = os.path.join(data_path,'test',test_order)\n",
        "        subset = list(pd.read_csv(test_csv_pth).file)\n",
        "        # subset = self.parse_csv(test_csv_pth)\n",
        "        test_order_list = subset# TODO: open test_order.csv as a list\n",
        "        self.X_names = [i for i in subset]# TODO: Load the npy files from test_order.csv and append into a list\n",
        "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[0])\n",
        "        return subset[0:]\n",
        "    def __len__(self):\n",
        "        return len(self.X_names)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        # TODOs: Need to return only X because this is the test dataset\n",
        "        X_path = os.path.join(self.data_path,'test','mfcc',self.X_names[ind])\n",
        "        X = torch.Tensor(np.load(X_path))\n",
        "        return X\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        batch_x = [x for x in batch]\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_x):\n",
        "            new_lst.append(batch_x[idx])\n",
        "        batch_x_pad = pad_sequence(new_lst)\n",
        "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "        # lengths_x_pad = [len(i) for i in batch_x_pad]\n",
        "        # batch_x = [x for x in batch]\n",
        "        # batch_x_pad = pad_sequence(batch_x)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        # lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vXy5nlN0EDS",
        "outputId": "54c40ebc-f4b0-429c-924b-f80720f87b06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.25062437e+01, -7.94386446e-01, -4.03778702e-01, ...,\n",
              "        -2.88804829e-01, -3.34463990e-03, -2.66719282e-01],\n",
              "       [ 8.21978569e+00, -1.09054469e-01, -3.57929736e-01, ...,\n",
              "        -6.92023411e-02, -2.47698724e-02,  2.29138322e-02],\n",
              "       [ 8.39872742e+00, -1.28472954e-01, -5.58285475e-01, ...,\n",
              "        -1.84500307e-01, -1.24606721e-01, -5.93495276e-03],\n",
              "       ...,\n",
              "       [ 8.45092773e+00, -8.12969580e-02, -8.61367941e-01, ...,\n",
              "        -1.45914614e-01,  2.78380603e-01,  8.75327587e-02],\n",
              "       [ 8.17366791e+00, -1.90407515e-01, -6.67871356e-01, ...,\n",
              "        -4.35884334e-02,  2.22096816e-01,  3.60963680e-02],\n",
              "       [ 8.28612709e+00, -2.09572345e-01, -8.19606900e-01, ...,\n",
              "         1.01721786e-01,  9.82918143e-02, -7.64896646e-02]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tsp = LibriSamplesTest(root,'test_order.csv')\n",
        "tsp.__getitem__(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4mzoYfTKu14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f3d0c1-a6eb-4abe-f85a-4bbb9b23fc01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 28539, batches = 223\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "num_classes = 41\n",
        "root = 'hw3p2_student_data/hw3p2_student_data' # TODO: Where your hw3p2_student_data folder is\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=train_data.collate_fn)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False,collate_fn=val_data.collate_fn)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=test_data.collate_fn)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u9FwVZ9I2da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7ed36f-fcc1-4722-d016-1f4c2f92f59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1688, 128, 13]) torch.Size([212, 128]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# Optional\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    # print(f\"ly {ly}\")\n",
        "    # print(f\"lx {lx}\")\n",
        "    lx = (lx/2).type(torch.LongTensor)\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZJhKBBV0EDT",
        "outputId": "4d75d29d-647e-41fe-833f-c59ae073f17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3003, 128, 13]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# for data in test_loader:\n",
        "#     x, lx = data\n",
        "#     print(x.shape, lx.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa6yAKbn0EDT",
        "outputId": "50a5c76b-4384-45da-967a-3070b0be4a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([824, 723, 424, 652, 684, 801, 583, 782, 720, 826, 363, 402, 709, 821,\n",
            "        785, 744, 576, 547, 790, 673, 807, 792, 522, 759, 156, 770, 771, 774,\n",
            "        643, 782, 616, 763, 678, 660, 618, 802, 745, 750, 595, 714, 778, 753,\n",
            "        719, 669, 608, 784, 552, 546, 637, 805, 656, 162, 805, 422, 752, 781,\n",
            "        779, 573, 748, 677, 455, 775, 621, 577, 226, 713, 168, 754, 809, 654,\n",
            "        781, 760, 116, 759, 481, 730, 827, 734, 600, 626, 736, 706, 747, 596,\n",
            "        718, 799, 113, 556, 646, 337, 761, 763, 778, 692, 614, 838, 711, 526,\n",
            "        528, 211, 497, 749, 686, 191, 434, 542, 701, 755, 789, 767, 814, 727,\n",
            "        814, 818, 672, 812, 676, 734, 767, 651, 115, 641, 513, 734, 405, 681,\n",
            "        780, 724])\n"
          ]
        }
      ],
      "source": [
        "# a = (lx/2).type(torch.LongTensor)\n",
        "# print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Iniialize a residual block with two convolutions followed by batchnorm layers\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(256,256,kernel_size = 1,stride= 1)\n",
        "        self.conv2 = nn.Conv1d(256,256,kernel_size = 1,stride= 1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def convblock(self, x):\n",
        "        x = F.relu(self.batchnorm1(self.conv1(x)))\n",
        "        x = F.relu(self.batchnorm2(self.conv2(x)))\n",
        "        return x\n",
        "   \n",
        "    \"\"\"\n",
        "    Combine output with the original input\n",
        "    \"\"\"\n",
        "    def forward(self, x): return x + self.convblock(x) # skip connection"
      ],
      "metadata": {
        "id": "KFuy18u52Bot"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CGoiXd70tb5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00228e3a-3a17-453b-e08e-73a02c4e28bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (cnn1): Sequential(\n",
            "    (0): Conv1d(13, 256, kernel_size=(1,), stride=(1,))\n",
            "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res): Sequential(\n",
            "    (0): ResBlock(\n",
            "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): ResBlock(\n",
            "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): ResBlock(\n",
            "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): ResBlock(\n",
            "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "      (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (batchnorm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(256, 256, num_layers=4, dropout=0.4, bidirectional=True)\n",
            "  (classification): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4, inplace=False)\n",
            "    (3): Linear(in_features=2048, out_features=41, bias=True)\n",
            "  )\n",
            ")\n",
            "========================================================================================\n",
            "                                  Kernel Shape      Output Shape     Params  \\\n",
            "Layer                                                                         \n",
            "0_cnn1.Conv1d_0                   [13, 256, 1]  [128, 256, 1688]     3.584k   \n",
            "1_cnn1.BatchNorm1d_1                     [256]  [128, 256, 1688]      512.0   \n",
            "2_res.0.Conv1d_conv1             [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "3_res.0.BatchNorm1d_batchnorm1           [256]  [128, 256, 1688]      512.0   \n",
            "4_res.0.Conv1d_conv2             [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "5_res.0.BatchNorm1d_batchnorm2           [256]  [128, 256, 1688]      512.0   \n",
            "6_res.1.Conv1d_conv1             [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "7_res.1.BatchNorm1d_batchnorm1           [256]  [128, 256, 1688]      512.0   \n",
            "8_res.1.Conv1d_conv2             [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "9_res.1.BatchNorm1d_batchnorm2           [256]  [128, 256, 1688]      512.0   \n",
            "10_res.2.Conv1d_conv1            [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "11_res.2.BatchNorm1d_batchnorm1          [256]  [128, 256, 1688]      512.0   \n",
            "12_res.2.Conv1d_conv2            [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "13_res.2.BatchNorm1d_batchnorm2          [256]  [128, 256, 1688]      512.0   \n",
            "14_res.3.Conv1d_conv1            [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "15_res.3.BatchNorm1d_batchnorm1          [256]  [128, 256, 1688]      512.0   \n",
            "16_res.3.Conv1d_conv2            [256, 256, 1]  [128, 256, 1688]    65.792k   \n",
            "17_res.3.BatchNorm1d_batchnorm2          [256]  [128, 256, 1688]      512.0   \n",
            "18_lstm                                      -      [80742, 512]  5.783552M   \n",
            "19_classification.Linear_0         [512, 2048]  [844, 128, 2048]  1.050624M   \n",
            "20_classification.ReLU_1                     -  [844, 128, 2048]          -   \n",
            "21_classification.Dropout_2                  -  [844, 128, 2048]          -   \n",
            "22_classification.Linear_3          [2048, 41]    [844, 128, 41]    84.009k   \n",
            "\n",
            "                                   Mult-Adds  \n",
            "Layer                                         \n",
            "0_cnn1.Conv1d_0                    5.617664M  \n",
            "1_cnn1.BatchNorm1d_1                   256.0  \n",
            "2_res.0.Conv1d_conv1             110.624768M  \n",
            "3_res.0.BatchNorm1d_batchnorm1         256.0  \n",
            "4_res.0.Conv1d_conv2             110.624768M  \n",
            "5_res.0.BatchNorm1d_batchnorm2         256.0  \n",
            "6_res.1.Conv1d_conv1             110.624768M  \n",
            "7_res.1.BatchNorm1d_batchnorm1         256.0  \n",
            "8_res.1.Conv1d_conv2             110.624768M  \n",
            "9_res.1.BatchNorm1d_batchnorm2         256.0  \n",
            "10_res.2.Conv1d_conv1            110.624768M  \n",
            "11_res.2.BatchNorm1d_batchnorm1        256.0  \n",
            "12_res.2.Conv1d_conv2            110.624768M  \n",
            "13_res.2.BatchNorm1d_batchnorm2        256.0  \n",
            "14_res.3.Conv1d_conv1            110.624768M  \n",
            "15_res.3.BatchNorm1d_batchnorm1        256.0  \n",
            "16_res.3.Conv1d_conv2            110.624768M  \n",
            "17_res.3.BatchNorm1d_batchnorm2        256.0  \n",
            "18_lstm                            5.767168M  \n",
            "19_classification.Linear_0         1.048576M  \n",
            "20_classification.ReLU_1                   -  \n",
            "21_classification.Dropout_2                -  \n",
            "22_classification.Linear_3           83.968k  \n",
            "----------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params            7.452713M\n",
            "Trainable params        7.452713M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             897.517824M\n",
            "========================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Kernel Shape      Output Shape     Params  \\\n",
              "Layer                                                                         \n",
              "0_cnn1.Conv1d_0                   [13, 256, 1]  [128, 256, 1688]     3584.0   \n",
              "1_cnn1.BatchNorm1d_1                     [256]  [128, 256, 1688]      512.0   \n",
              "2_res.0.Conv1d_conv1             [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "3_res.0.BatchNorm1d_batchnorm1           [256]  [128, 256, 1688]      512.0   \n",
              "4_res.0.Conv1d_conv2             [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "5_res.0.BatchNorm1d_batchnorm2           [256]  [128, 256, 1688]      512.0   \n",
              "6_res.1.Conv1d_conv1             [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "7_res.1.BatchNorm1d_batchnorm1           [256]  [128, 256, 1688]      512.0   \n",
              "8_res.1.Conv1d_conv2             [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "9_res.1.BatchNorm1d_batchnorm2           [256]  [128, 256, 1688]      512.0   \n",
              "10_res.2.Conv1d_conv1            [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "11_res.2.BatchNorm1d_batchnorm1          [256]  [128, 256, 1688]      512.0   \n",
              "12_res.2.Conv1d_conv2            [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "13_res.2.BatchNorm1d_batchnorm2          [256]  [128, 256, 1688]      512.0   \n",
              "14_res.3.Conv1d_conv1            [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "15_res.3.BatchNorm1d_batchnorm1          [256]  [128, 256, 1688]      512.0   \n",
              "16_res.3.Conv1d_conv2            [256, 256, 1]  [128, 256, 1688]    65792.0   \n",
              "17_res.3.BatchNorm1d_batchnorm2          [256]  [128, 256, 1688]      512.0   \n",
              "18_lstm                                      -      [80742, 512]  5783552.0   \n",
              "19_classification.Linear_0         [512, 2048]  [844, 128, 2048]  1050624.0   \n",
              "20_classification.ReLU_1                     -  [844, 128, 2048]        NaN   \n",
              "21_classification.Dropout_2                  -  [844, 128, 2048]        NaN   \n",
              "22_classification.Linear_3          [2048, 41]    [844, 128, 41]    84009.0   \n",
              "\n",
              "                                   Mult-Adds  \n",
              "Layer                                         \n",
              "0_cnn1.Conv1d_0                    5617664.0  \n",
              "1_cnn1.BatchNorm1d_1                   256.0  \n",
              "2_res.0.Conv1d_conv1             110624768.0  \n",
              "3_res.0.BatchNorm1d_batchnorm1         256.0  \n",
              "4_res.0.Conv1d_conv2             110624768.0  \n",
              "5_res.0.BatchNorm1d_batchnorm2         256.0  \n",
              "6_res.1.Conv1d_conv1             110624768.0  \n",
              "7_res.1.BatchNorm1d_batchnorm1         256.0  \n",
              "8_res.1.Conv1d_conv2             110624768.0  \n",
              "9_res.1.BatchNorm1d_batchnorm2         256.0  \n",
              "10_res.2.Conv1d_conv1            110624768.0  \n",
              "11_res.2.BatchNorm1d_batchnorm1        256.0  \n",
              "12_res.2.Conv1d_conv2            110624768.0  \n",
              "13_res.2.BatchNorm1d_batchnorm2        256.0  \n",
              "14_res.3.Conv1d_conv1            110624768.0  \n",
              "15_res.3.BatchNorm1d_batchnorm1        256.0  \n",
              "16_res.3.Conv1d_conv2            110624768.0  \n",
              "17_res.3.BatchNorm1d_batchnorm2        256.0  \n",
              "18_lstm                            5767168.0  \n",
              "19_classification.Linear_0         1048576.0  \n",
              "20_classification.ReLU_1                 NaN  \n",
              "21_classification.Dropout_2              NaN  \n",
              "22_classification.Linear_3           83968.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-980f4672-1500-4e6b-a356-f7e882e42dc8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_cnn1.Conv1d_0</th>\n",
              "      <td>[13, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>3584.0</td>\n",
              "      <td>5617664.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_cnn1.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_res.0.Conv1d_conv1</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_res.0.BatchNorm1d_batchnorm1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_res.0.Conv1d_conv2</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_res.0.BatchNorm1d_batchnorm2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_res.1.Conv1d_conv1</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_res.1.BatchNorm1d_batchnorm1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_res.1.Conv1d_conv2</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_res.1.BatchNorm1d_batchnorm2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_res.2.Conv1d_conv1</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_res.2.BatchNorm1d_batchnorm1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_res.2.Conv1d_conv2</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_res.2.BatchNorm1d_batchnorm2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_res.3.Conv1d_conv1</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_res.3.BatchNorm1d_batchnorm1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_res.3.Conv1d_conv2</th>\n",
              "      <td>[256, 256, 1]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>65792.0</td>\n",
              "      <td>110624768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_res.3.BatchNorm1d_batchnorm2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[128, 256, 1688]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[80742, 512]</td>\n",
              "      <td>5783552.0</td>\n",
              "      <td>5767168.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_classification.Linear_0</th>\n",
              "      <td>[512, 2048]</td>\n",
              "      <td>[844, 128, 2048]</td>\n",
              "      <td>1050624.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_classification.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[844, 128, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_classification.Dropout_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[844, 128, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_classification.Linear_3</th>\n",
              "      <td>[2048, 41]</td>\n",
              "      <td>[844, 128, 41]</td>\n",
              "      <td>84009.0</td>\n",
              "      <td>83968.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-980f4672-1500-4e6b-a356-f7e882e42dc8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-980f4672-1500-4e6b-a356-f7e882e42dc8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-980f4672-1500-4e6b-a356-f7e882e42dc8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self): # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        # self.embedding = \n",
        "        # self.cnn = nn.Sequential(nn.Conv1d(13,128,kernel_size = 1,stride= 2),nn.BatchNorm1d(128),nn.Conv1d(128,256,kernel_size = 1,stride= 1),nn.BatchNorm1d(256))\n",
        "        self.cnn1 = nn.Sequential(nn.Conv1d(13,256,kernel_size = 1,stride= 1),nn.BatchNorm1d(256))\n",
        "        self.res = nn.Sequential(*[ResBlock() for i in range(4)])\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=256,hidden_size= 256,bidirectional =True, num_layers= 4,dropout=0.4)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "        self.classification = nn.Sequential(nn.Linear(256*2,2048),nn.ReLU(),nn.Dropout(p=0.4),nn.Linear(2048,41))# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "    def forward(self, x, lx): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
        "\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        # print(x.shape)\n",
        "        x = self.cnn1(x.permute(1,2,0))\n",
        "        x = self.res(x)\n",
        "        x = x.permute(2,0,1)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        \n",
        "        packed_input = pack_padded_sequence(x,lx,enforce_sorted=False)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
        "\n",
        "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        out, lengths  = pad_packed_sequence(out1)# TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "        \n",
        "        # out = out.permute(1,0,2)\n",
        "        # print(out.shape)\n",
        "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
        "        # out = # Optional: Do log softmax on the output. Which dimension?\n",
        "        # print(out[0,0,:])\n",
        "        out = torch.nn.functional.log_softmax(out,dim=2)\n",
        "        # print(out[0,0,:])\n",
        "        # print(sum(out[0,0,:]))\n",
        "        return out, lengths # TODO: Need to return 2 variables\n",
        "\n",
        "model = Network()\n",
        "print(model)\n",
        "summary(model, x, lx) # x and lx are from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rBnuSa8u0EDU"
      },
      "outputs": [],
      "source": [
        "# this function calculates the Levenshtein distance \n",
        "\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
        "\n",
        "    # h - ouput from the model. Probability distributions at each time step \n",
        "    # y - target output sequence - sequence of Long tensors\n",
        "    # lh, ly - Lengths of output and target\n",
        "    # decoder - decoder object which was initialized in the previous cell\n",
        "    # PHONEME_MAP - maps output to a character to find the Levenshtein distance\n",
        "\n",
        "    h = h.permute(1, 0, 2)# TODO: You may need to transpose or permute h based on how you passed it to the criterion\n",
        "    # Print out the shapes often to debug\n",
        "    t1 =time.time()\n",
        "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
        "    t2 = time.time()\n",
        "    # print(f\"time cost {t2-t1}\")\n",
        "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
        "    # Input to the decode method will be h and its lengths lh \n",
        "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
        "\n",
        "    batch_size = h.shape[0]# TODO\n",
        "    # print(f\"batch_szie {batch_size}\")\n",
        "\n",
        "    dist = 0\n",
        "\n",
        "    # dist = 0\n",
        "    # h = np.zeros((100,))  \n",
        "    # y = y.cpu().detach().numpy().astype(int)\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "    # for j in range(100)\n",
        "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
        "    # print(h_sliced.shape)\n",
        "        # TODO: Get the output as a sequence of numbers from beam_results\n",
        "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
        "        # Same goes for beam_results and out_lens\n",
        "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
        "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
        "\n",
        "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        # print(f\"ly.shape {ly.shape}\")\n",
        "        # print(f\"y.shape {y.shape}\")\n",
        "        y_sliced = y[i][:ly[i]]\n",
        "        y_string = \"\".join([PHONEME_MAP[j] for j in y_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        # print(f\"h_string {h_string} h_string.len {len(h_string)}\")\n",
        "        # print(f\"y_string {y_string} y_string.len {len(y_string)}\")\n",
        "        per_dist = Levenshtein.distance(h_string, y_string)\n",
        "        # print(f\"{i} {per_dist} \")\n",
        "        dist += per_dist\n",
        "\n",
        "    dist/=batch_size\n",
        "    return dist\n",
        "    # print(f\"dist {dist}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kv84Ni00EDW",
        "outputId": "4f916b5c-2a4f-49c8-d831-b382cba8d9e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 28539, batches = 446\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "lr = 2e-3\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "num_classes = 41\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": epochs,\n",
        "  \"batch_size\": batch_size\n",
        "}\n",
        "\n",
        "root = 'hw3p2_student_data/hw3p2_student_data' # TODO: Where your hw3p2_student_data folder is\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=train_data.collate_fn, num_workers=8)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False,collate_fn=val_data.collate_fn, num_workers=8)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=test_data.collate_fn, num_workers=8)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "criterion = nn.CTCLoss()# TODO: What loss do you need for sequence to sequence models? \n",
        "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)# TODO: Adam works well with LSTM (use lr = 2e-3)\n",
        "# optimizer = torch.optim.SGD(model.parameters(),lr=0.02,momentum=0.9)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "decoder = ctcdecode.CTCBeamDecoder(labels = PHONEME_MAP,\n",
        "    model_path=None,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=40,\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=5,\n",
        "    num_processes=8,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True)# TODO: Intialize the CTC beam decoder\n",
        "# Check out https://github.com/parlance/ctcdecode for the details on how to implement decoding\n",
        "# Do you need to give log_probs_input = True or False?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Q5npQNFH315V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "0e0cc612-06a4-440b-c68c-4d72021bdfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch: 1:   3%|▎         | 13/446 [00:26<13:47,  1.91s/it, loss=7.4313, lr=0.0020]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0f331a287c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.04f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# torch.cuda.empty_cache() # Use this often\n",
        "\n",
        "# TODO: Write the model evaluation function if you want to validate after every epoch\n",
        "\n",
        "# You are free to write your own code for model evaluation or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Note that when you give a higher beam width, decoding will take a longer time to get executed\n",
        "# Therefore, it is recommended that you calculate only the val dataset's Levenshtein distance (train not recommended) with a small beam width\n",
        "# When you are evaluating on your test set, you may have a higher beam width\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# model.load_state_dict(torch.load(\"7_1.0379955768585205_25_03_2022_22_30_06model.pth\"))\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # num_correct = 0\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=f'Train epoch: {epoch+1}') \n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    desc = \"start\"\n",
        "    # tq = tqdm(train_loader, desc=desc,dynamic_ncols=True)\n",
        "    # with tqdm(train_loader, desc=desc,dynamic_ncols=True) as tq:\n",
        "    for i, (x, y, lx, ly) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        # lx = (lx/2).type(torch.LongTensor)\n",
        "        out, out_len = model(x,lx)\n",
        "        loss = criterion(out,y.permute(1,0), lx, ly)\n",
        "        total_loss += loss\n",
        "        # desc = \"loss = {:.04f}\".format(float(total_loss / (i + 1)))\n",
        "        # desc += \"lr = {:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        # tq.update()\n",
        "        batch_bar.update() \n",
        "    batch_bar.close()\n",
        "    now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    torch.save(model.state_dict(), f\"{epoch}_{float(total_loss / len(train_loader))}_{now_name}model.pth\")\n",
        "    tqdm.write(\"Epoch {}/{}: Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "    wandb.log({\"Train Loss\": float(total_loss / len(train_loader))})\n",
        "    wandb.log({\"lr\" : float(optimizer.param_groups[0]['lr'])})\n",
        "\n",
        "    # if epoch%5==0:\n",
        "    model.eval()\n",
        "    \n",
        "    t_val_loss = 0\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        spectrograms, labels, input_lengths, label_lengths = data\n",
        "        # print(f\"label_lengths {label_lengths}\")\n",
        "        spectrograms, labels =spectrograms.to(device), labels\n",
        "        # input_lengths = (input_lengths/2).type(torch.LongTensor)\n",
        "        with torch.no_grad():\n",
        "            out,out_lengths = model(spectrograms,input_lengths)\n",
        "        t_val_loss += criterion(out,labels.permute(1,0), input_lengths, label_lengths)\n",
        "        \n",
        "    val_loss = t_val_loss/len(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "    tqdm.write(\"Epoch {}/{}: val loss {:.04f}\".format(\n",
        "            epoch + 1,\n",
        "            epochs,\n",
        "            float(val_loss),\n",
        "            ))\n",
        "    \n",
        "    wandb.log({\"val_loss\" : float(val_loss)})\n",
        "    if epoch%5==0:\n",
        "        t_dist = 0\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            spectrograms, labels, input_lengths, label_lengths = data\n",
        "            # print(f\"label_lengths {label_lengths}\")\n",
        "            spectrograms, labels =spectrograms.to(device), labels\n",
        "            # input_lengths = (input_lengths/2).type(torch.LongTensor)\n",
        "            with torch.no_grad():\n",
        "                out,out_lengths = model(spectrograms,input_lengths)\n",
        "            t_dist = calculate_levenshtein(out, labels.permute(1,0), out_lengths, label_lengths, decoder, PHONEME_MAP)\n",
        "            break\n",
        "        # dist = t_dist/len(val_loader)\n",
        "        wandb.log({\"dist\" : float(t_dist)})\n",
        "        tqdm.write(\"distance {:.04f}\".format(\n",
        "            float(t_dist),\n",
        "            ))\n",
        "            # break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0-rGigf0EDX"
      },
      "outputs": [],
      "source": [
        "# Optional but recommended\n",
        "# model.to(device)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    spectrograms, labels, input_lengths, label_lengths = data\n",
        "    # print(f\"label_lengths {label_lengths}\")\n",
        "    spectrograms, labels =spectrograms.to(device), labels\n",
        "    out,out_lengths = model(spectrograms,input_lengths)\n",
        "    # print(f\"out_lengths {out_lengths}\")\n",
        "    # print(out.shape)\n",
        "    # oss = criterion(out,labels.permute(1,0), input_lengths, label_lengths)\n",
        "    # out = model(spectrograms,input_lengths)\n",
        "    # print(out.shape)\n",
        "    loss = criterion(out,labels.permute(1,0), out_lengths, label_lengths)\n",
        "    # print(labels)\n",
        "    calculate_levenshtein(out, labels.permute(1,0), out_lengths, label_lengths, decoder, PHONEME_MAP)\n",
        "    # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
        "    # Make sure that you are able to get this right before going on to the actual training\n",
        "    # You may encounter a lot of shape errors\n",
        "    # Printing out the shapes will help in debugging\n",
        "    # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
        "    # Make sure to read the corresponding docs about it\n",
        "    # pass\n",
        "\n",
        "    break # one iteration is enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ8kac_U0EDX"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\"6_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: Write the model training code \n",
        "\n",
        "\n",
        "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Tip: Implement mixed precision training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrqXnNqzJSc"
      },
      "source": [
        "# Submit to kaggle (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGeWqF9y0EDY"
      },
      "outputs": [],
      "source": [
        "def pred(h,lh,decoder,PHONEME_MAP):\n",
        "    h = h.permute(1, 0, 2)\n",
        "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
        "    h_string_lst = []\n",
        "    batch_size = h.shape[0]\n",
        "    for i in range(batch_size): \n",
        "\n",
        "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
        "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])\n",
        "        h_string_lst.append(h_string)\n",
        "    return h_string_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your model evaluation code for the test dataset\n",
        "# You can write your own code or use from the previous homewoks' stater notebooks\n",
        "# You can't calculate loss here. Why?\n",
        "model = model.to(device)\n",
        "def submit_test(model):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(1):\n",
        "            # X = test_samples[i]\n",
        "\n",
        "            # test_items = test_item(X, context=args['context'])\n",
        "            # test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for x, lx in test_loader:\n",
        "                # data = data.float().to(device)\n",
        "                x = x.cuda()\n",
        "                # y = y.cuda()\n",
        "                lx = (lx/2).type(torch.LongTensor)\n",
        "                out, out_len = model(x,lx)\n",
        "                pred_y = pred(out,out_len,decoder,PHONEME_MAP)\n",
        "                # print(out.shape)\n",
        "                # pred_y = torch.argmax(out, axis=2)\n",
        "                # print(pred_y.shape)\n",
        "                pred_y_list.extend(pred_y)\n",
        "    # print(pred_y_list)\n",
        "    # now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    f = open(f\"bad.csv\", \"w\")\n",
        "    f.write(\"id,predictions\\n\")\n",
        "    for idx, i  in enumerate(pred_y_list):\n",
        "        f.write(f\"{idx},{i}\\n\")\n",
        "    f.close()\n",
        "\n",
        " \n",
        "    # with open('good.csv', 'w', newline='') as csvfile:\n",
        "    #     writer = csv.DictWriter(csvfile, fieldnames = ['id','label'])\n",
        "    #     # writer.writerow(['id','label'])\n",
        "    #     writer.writeheader() \n",
        "    #     for idx, i  in enumerate(pred_y_list):\n",
        "    #         writer.writerow([idx,i])\n",
        "        \n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the csv file\n",
        "submit_test(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xidif94W0EDY",
        "outputId": "c15ab6ba-a781-4fd2-e6bc-08c9ea00ecac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/linjiw/.kaggle/kaggle.json'\n",
            "100%|█████████████████████████████████████████| 212k/212k [00:00<00:00, 218kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "! kaggle competitions submit -c 11-785-s22-hw3p2 -f bad.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HW3P2_Starter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}