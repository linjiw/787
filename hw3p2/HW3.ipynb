{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykh8vQc0p71n"
      },
      "source": [
        "# 11785 HW3P2: Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0m4w_KkMeQ"
      },
      "source": [
        "Welcome to HW3P2. In this homework, you will be using the same data from HW1 but will be incorporating sequence models. We recommend you get familaried with sequential data and the working of RNNs, LSTMs and GRUs to have a smooth learning in this part of the homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEll1kGkhcR"
      },
      "source": [
        "Disclaimer: This starter notebook will not be as elaborate as that of HW1P2 or HW2P2. You will need to do most of the implementation in this notebook because, it is expected after 2 HWs, you will be in a position to write a notebook from scratch. You are welcomed to reuse the code from the previous starter notebooks but may also need to make appropriate changes for this homework. <br>\n",
        "We have also given you 3 log files for the Very Low Cutoff (Levenshtein Distance = 30) so that you can observe how loss decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHGaJ_8tx_5Z"
      },
      "source": [
        "Common errors which you may face\n",
        "\n",
        "\n",
        "*   Shape errors: Half of the errors from this homework will account to this category. Try printing the shapes between intermediate steps to debug\n",
        "*   CUDA out of Memory: When your architecture has a lot of parameters, this can happen. Golden keys for this is, (1) Reducing batch_size (2) Call *torch.cuda.empty_cache* often, even inside your training loop, (3) Call *gc.collect* if it helps and (4) Restart run time if nothing works\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fwJWcpqJDR"
      },
      "source": [
        "# Prelimilaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leymyQ-apwT6"
      },
      "source": [
        "You will need to install packages for decoding and calculating the Levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZCQtZtkaTrcn"
      },
      "outputs": [],
      "source": [
        "# !pip install python-Levenshtein\n",
        "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "# !pip install wget\n",
        "# %cd ctcdecode\n",
        "# !pip install .\n",
        "# %cd ..\n",
        "\n",
        "# !pip install torchsummaryX # We also install a summary package to check our model's forward before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "c8f8d454-1e76-492e-dcf9-3e2d16d38b35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinjiw\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/linjiw/Downloads/hw3/tmp/wandb/run-20220326_143004-39anlsry</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/linjiw/HW3/runs/39anlsry\" target=\"_blank\">serene-glade-15</a></strong> to <a href=\"https://wandb.ai/linjiw/HW3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from os.path import join\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import phonemes\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "import csv\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "# from tqdm import tqdm_notebook as tqdm\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"HW3\", entity=\"linjiw\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "# !jupyter nbextension enable --py widgetsnbextension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6gTI0Rslxrr"
      },
      "source": [
        "You need to set up your Kaggle and download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TPBUd7Cnl-Rx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s22-hw3p2.zip to /home/linjiw/Downloads/hw3/HW3 Starter\n",
            "100%|█████████████████████████████████████▉| 1.84G/1.84G [01:49<00:00, 15.5MB/s]\n",
            "100%|██████████████████████████████████████| 1.84G/1.84G [01:49<00:00, 18.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ! kaggle competitions download -c 11-785-s22-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if2Somqfbje1"
      },
      "outputs": [],
      "source": [
        "# ! unzip 11-785-s22-hw3p2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUCKqm1ST1sU"
      },
      "source": [
        "# Dataset and dataloading (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-PjVxPCBvVR6"
      },
      "outputs": [],
      "source": [
        "# PHONEME_MAP is the list that maps the phoneme to a single character. \n",
        "# The dataset contains a list of phonemes but you need to map them to their corresponding characters to calculate the Levenshtein Distance\n",
        "# You final submission should not have the phonemes but the mapped string\n",
        "# No TODOs in this cell\n",
        "\n",
        "PHONEME_MAP = [\n",
        "    \" \",\n",
        "    \".\", #SIL\n",
        "    \"a\", #AA\n",
        "    \"A\", #AE\n",
        "    \"h\", #AH\n",
        "    \"o\", #AO\n",
        "    \"w\", #AW\n",
        "    \"y\", #AY\n",
        "    \"b\", #B\n",
        "    \"c\", #CH\n",
        "    \"d\", #D\n",
        "    \"D\", #DH\n",
        "    \"e\", #EH\n",
        "    \"r\", #ER\n",
        "    \"E\", #EY\n",
        "    \"f\", #F\n",
        "    \"g\", #G\n",
        "    \"H\", #H\n",
        "    \"i\", #IH \n",
        "    \"I\", #IY\n",
        "    \"j\", #JH\n",
        "    \"k\", #K\n",
        "    \"l\", #L\n",
        "    \"m\", #M\n",
        "    \"n\", #N\n",
        "    \"N\", #NG\n",
        "    \"O\", #OW\n",
        "    \"Y\", #OY\n",
        "    \"p\", #P \n",
        "    \"R\", #R\n",
        "    \"s\", #S\n",
        "    \"S\", #SH\n",
        "    \"t\", #T\n",
        "    \"T\", #TH\n",
        "    \"u\", #UH\n",
        "    \"U\", #UW\n",
        "    \"v\", #V\n",
        "    \"W\", #W\n",
        "    \"?\", #Y\n",
        "    \"z\", #Z\n",
        "    \"Z\" #ZH\n",
        "]\n",
        "phe_dict = {}\n",
        "tensor_dict = {}\n",
        "PHONEMES = phonemes.PHONEMES\n",
        "for idx, i in enumerate(PHONEMES):\n",
        "    phe_dict[i] = PHONEME_MAP[idx]\n",
        "    tensor_dict[PHONEME_MAP[idx]] = idx\n",
        "\n",
        "def maplst(lst):\n",
        "    res =[]\n",
        "    for i in lst:\n",
        "        res.append(phe_dict[i])\n",
        "    res = np.array(res)\n",
        "    # res = res.astype(np.float)\n",
        "    return np.array(res)\n",
        "def maptotensor(lst):\n",
        "    res =[]\n",
        "    for i in lst:\n",
        "        res.append(tensor_dict[i])\n",
        "    res = np.array(res)\n",
        "    # res = res.astype(np.float)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['b' 'i' 'k' 'S' 'a']\n",
            "[ 8 18 21 31  2]\n"
          ]
        }
      ],
      "source": [
        "lst = ['B', 'IH', 'K', 'SH', 'AA']\n",
        "res = maplst(lst)\n",
        "tsr = maptotensor(res)\n",
        "print(res)\n",
        "print(tsr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is where your actual TODOs start\n",
        "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
        "# The steps for implementation given below are how we have implemented it.\n",
        "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_dir = os.path.join(data_path,partition,\"mfcc/\")# TODO: get mfcc directory path\n",
        "        self.Y_dir = os.path.join(data_path,partition,\"transcript/\")# TODO: get transcript path\n",
        "\n",
        "        self.X_files = os.listdir(self.X_dir)# TODO: list files in the mfcc directory\n",
        "        self.Y_files = os.listdir(self.Y_dir)# TODO: list files in the transcript directory\n",
        "\n",
        "        # TODO: store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
        "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
        "        self.PHONEMES = phonemes.PHONEMES\n",
        "\n",
        "        assert(len(self.X_files) == len(self.Y_files))\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        X_path = self.X_dir + self.X_files[ind]\n",
        "        Y_path = self.Y_dir + self.Y_files[ind]\n",
        "        X = torch.Tensor(np.load(X_path))# TODO: Load the mfcc npy file at the specified index ind in the directory\n",
        "        # Y = maplst(np.load(Y_path)[1:-1])# TODO: Load the corresponding transcripts\n",
        "        Y = np.load(Y_path)[1:-1]\n",
        "        # print(Y)\n",
        "        # print(Y.shape)\n",
        "        # Y2 = PHONEMES.index(i) for i in np.load(Y_path)[1:-1]\n",
        "        # print(f\"Y {Y}\")\n",
        "        # print(f\"Y2 {Y2}\")\n",
        "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
        "        # You need to convert these into a sequence of Long tensors\n",
        "        # Tip: You may need to use self.PHONEMES\n",
        "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them. \n",
        "        # You need to remove '<sos>' and '<eos>' from the trancripts. \n",
        "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
        "        Yy = torch.LongTensor([PHONEMES.index(i) for i in Y])\n",
        "        # Yy = torch.Tensor(maptotensor(Y)).type(torch.LongTensor)# TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "        # print(f\"X {X.shape}\")\n",
        "        # print(f\"Y {Y.shape}\")\n",
        "        return X, Yy\n",
        "    \n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x = [x for x,y in batch]\n",
        "        batch_y = [y for x,y in batch]\n",
        "        # print(batch_x[0].shape)\n",
        "\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_x):\n",
        "            new_lst.append(batch_x[idx])\n",
        "        batch_x_pad = pad_sequence(new_lst)\n",
        "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "        lengths_x_pad = [len(i) for i in batch_x_pad]\n",
        "        # print(f\"batch_x {batch_x}\")\n",
        "        \n",
        "\n",
        "        # print(f\"test_pad.len {test_pad.shape}\")\n",
        "        # print(f\"batch_x.len {len(batch_x)}\")\n",
        "        # print(f\"lengths_x {lengths_x}\")\n",
        "        # print(f\"lengths_x_pad {lengths_x_pad}\")\n",
        "\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_y):\n",
        "            new_lst.append(batch_y[idx])\n",
        "        batch_y_pad = pad_sequence(new_lst)\n",
        "\n",
        "        # batch_y_pad = pad_sequence(batch_y) # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = [len(i) for i in batch_y] # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = 'hw3p2_student_data/hw3p2_student_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X torch.Size([1428, 13])\n",
            "Y (130,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.2427e+01, -1.1178e+00, -2.4150e-01,  ..., -2.0521e-03,\n",
              "          -1.5512e-01, -1.6788e-01],\n",
              "         [ 7.2623e+00, -1.0281e-01, -1.1216e+00,  ..., -1.3412e-01,\n",
              "          -9.4587e-02,  1.3803e-01],\n",
              "         [ 6.7271e+00, -5.3644e-02, -8.4255e-01,  ...,  1.2032e-02,\n",
              "          -9.2675e-02, -8.1234e-02],\n",
              "         ...,\n",
              "         [ 5.5238e+00, -6.6449e-01, -2.4509e-01,  ...,  7.9069e-02,\n",
              "           7.8216e-02,  1.0809e-01],\n",
              "         [ 5.5423e+00, -4.0494e-01, -2.4527e-01,  ..., -2.4723e-02,\n",
              "          -1.0742e-01, -7.8955e-02],\n",
              "         [ 5.8132e+00, -4.8034e-01, -4.6667e-01,  ..., -6.9344e-02,\n",
              "          -5.6162e-02,  9.8189e-02]]),\n",
              " tensor([ 1., 37., 18.,  9.,  1., 12., 24., 10., 18., 10., 18., 24.,  9.,  3.,\n",
              "         24., 32., 19., 39.,  1.,  4., 32., 13., 10., 18., 15., 19., 32.,  1.,\n",
              "         15.,  5., 29., 17., 19., 10.,  7., 10., 15., 13., 23., 17., 18., 39.,\n",
              "         37., 35., 24., 10., 39.,  1., 10.,  6., 24., 19.,  3., 24., 10., 30.,\n",
              "         24., 26.,  8.,  5., 22., 30., 35., 24., 15.,  2., 22., 26., 10.,  1.,\n",
              "         15.,  5., 29., 11.,  4., 32., 35., 30., 37., 19., 32.,  1., 22., 18.,\n",
              "         32.,  4., 22., 33., 18., 25., 39., 37., 34., 10., 30., 37., 18., 25.,\n",
              "          2., 24., 11.,  4.,  8.,  4., 29., 10.,  4., 21., 22., 19., 36., 39.,\n",
              "          1., 11.,  4., 32., 16., 29., 35., 26., 36., 13., 11.,  4.,  8., 29.,\n",
              "         34., 21.,  1.,  1.]))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lsmp = LibriSamples(root,\"train\")\n",
        "lsmp.__getitem__(1)\n",
        "# lsmp.collate_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, test_order): # test_order is the csv similar to what you used in hw1\n",
        "        self.data_path = data_path\n",
        "        test_csv_pth = os.path.join(data_path,'test',test_order)\n",
        "        subset = list(pd.read_csv(test_csv_pth).file)\n",
        "        # subset = self.parse_csv(test_csv_pth)\n",
        "        test_order_list = subset# TODO: open test_order.csv as a list\n",
        "        self.X_names = [i for i in subset]# TODO: Load the npy files from test_order.csv and append into a list\n",
        "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[0])\n",
        "        return subset[0:]\n",
        "    def __len__(self):\n",
        "        return len(self.X_names)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        # TODOs: Need to return only X because this is the test dataset\n",
        "        X_path = os.path.join(self.data_path,'test','mfcc',self.X_names[ind])\n",
        "        X = torch.Tensor(np.load(X_path))\n",
        "        return X\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        batch_x = [x for x in batch]\n",
        "        new_lst = []\n",
        "        for idx, i in enumerate(batch_x):\n",
        "            new_lst.append(batch_x[idx])\n",
        "        batch_x_pad = pad_sequence(new_lst)\n",
        "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "        # lengths_x_pad = [len(i) for i in batch_x_pad]\n",
        "        # batch_x = [x for x in batch]\n",
        "        # batch_x_pad = pad_sequence(batch_x)# TODO: pad the sequence with pad_sequence (already imported)\n",
        "        # lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.25062437e+01, -7.94386446e-01, -4.03778702e-01, ...,\n",
              "        -2.88804829e-01, -3.34463990e-03, -2.66719282e-01],\n",
              "       [ 8.21978569e+00, -1.09054469e-01, -3.57929736e-01, ...,\n",
              "        -6.92023411e-02, -2.47698724e-02,  2.29138322e-02],\n",
              "       [ 8.39872742e+00, -1.28472954e-01, -5.58285475e-01, ...,\n",
              "        -1.84500307e-01, -1.24606721e-01, -5.93495276e-03],\n",
              "       ...,\n",
              "       [ 8.45092773e+00, -8.12969580e-02, -8.61367941e-01, ...,\n",
              "        -1.45914614e-01,  2.78380603e-01,  8.75327587e-02],\n",
              "       [ 8.17366791e+00, -1.90407515e-01, -6.67871356e-01, ...,\n",
              "        -4.35884334e-02,  2.22096816e-01,  3.60963680e-02],\n",
              "       [ 8.28612709e+00, -2.09572345e-01, -8.19606900e-01, ...,\n",
              "         1.01721786e-01,  9.82918143e-02, -7.64896646e-02]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tsp = LibriSamplesTest(root,'test_order.csv')\n",
        "tsp.__getitem__(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 28539, batches = 223\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 128\n",
        "# num_classes = 41\n",
        "# root = 'hw3p2_student_data/hw3p2_student_data' # TODO: Where your hw3p2_student_data folder is\n",
        "\n",
        "# train_data = LibriSamples(root, 'train')\n",
        "# val_data = LibriSamples(root, 'dev')\n",
        "# test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "# train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=train_data.collate_fn)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "# val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False,collate_fn=val_data.collate_fn)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "# test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=test_data.collate_fn)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "# print(\"Batch size: \", batch_size)\n",
        "# print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "# print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "# print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u9FwVZ9I2da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ly tensor([172, 148,  91, 137, 160, 168, 120, 180, 130, 191,  71,  83, 144, 160,\n",
            "        183, 159, 132,  99, 183, 141, 179, 141, 120, 159,  22, 147, 185, 178,\n",
            "        146, 165, 157, 156, 140, 109, 134, 148, 171, 160, 128, 141, 141, 116,\n",
            "        143, 136, 159, 173, 133, 124, 123, 207, 158,  28, 139,  88, 144, 177,\n",
            "        177, 122, 143, 140,  87, 145, 140, 144,  44, 160,  42, 128, 139,  94,\n",
            "        147, 161,  20, 153,  95, 120, 171, 143, 113, 116, 168, 159, 135, 149,\n",
            "        137, 160,  28, 119, 139,  89, 161, 143, 127, 157, 148, 152, 145, 126,\n",
            "        108,  48,  74, 167, 163,  34,  90, 120, 135, 162, 156, 186, 153, 117,\n",
            "        173, 211, 143, 141, 151, 172, 128, 154,  30, 118,  96, 145, 100, 180,\n",
            "        168, 168])\n",
            "lx tensor([1648, 1447,  848, 1304, 1368, 1603, 1167, 1565, 1441, 1652,  726,  805,\n",
            "        1418, 1642, 1570, 1489, 1152, 1094, 1581, 1347, 1615, 1585, 1044, 1519,\n",
            "         312, 1541, 1543, 1549, 1287, 1565, 1233, 1527, 1357, 1321, 1237, 1604,\n",
            "        1490, 1500, 1190, 1429, 1556, 1506, 1439, 1339, 1217, 1569, 1105, 1092,\n",
            "        1275, 1611, 1313,  324, 1611,  845, 1505, 1562, 1559, 1146, 1496, 1355,\n",
            "         911, 1550, 1242, 1155,  452, 1427,  337, 1509, 1619, 1308, 1562, 1521,\n",
            "         233, 1519,  962, 1460, 1654, 1468, 1200, 1253, 1472, 1412, 1495, 1192,\n",
            "        1437, 1599,  226, 1112, 1293,  675, 1522, 1527, 1557, 1385, 1229, 1677,\n",
            "        1422, 1053, 1056,  423,  994, 1498, 1373,  383,  868, 1084, 1402, 1510,\n",
            "        1578, 1535, 1629, 1455, 1628, 1636, 1345, 1625, 1353, 1469, 1535, 1303,\n",
            "         231, 1282, 1026, 1468,  810, 1362, 1560, 1449])\n",
            "torch.Size([1677, 128, 13]) torch.Size([211, 128]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# # Optional\n",
        "# # Test code for checking shapes and return arguments of the train and val loaders\n",
        "# for data in train_loader:\n",
        "#     x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "#     print(f\"ly {ly}\")\n",
        "#     print(f\"lx {lx}\")\n",
        "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3003, 128, 13]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# for data in test_loader:\n",
        "#     x, lx = data\n",
        "#     print(x.shape, lx.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (lstm): LSTM(13, 256)\n",
            "  (classification): Linear(in_features=256, out_features=41, bias=True)\n",
            ")\n",
            "=================================================================\n",
            "                 Kernel Shape     Output Shape  Params  Mult-Adds\n",
            "Layer                                                            \n",
            "0_lstm                      -    [325370, 256]  277504     275456\n",
            "1_classification    [256, 41]  [1709, 256, 41]   10537      10496\n",
            "-----------------------------------------------------------------\n",
            "                      Totals\n",
            "Total params          288041\n",
            "Trainable params      288041\n",
            "Non-trainable params       0\n",
            "Mult-Adds             285952\n",
            "=================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[325370, 256]</td>\n",
              "      <td>277504</td>\n",
              "      <td>275456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_classification</th>\n",
              "      <td>[256, 41]</td>\n",
              "      <td>[1709, 256, 41]</td>\n",
              "      <td>10537</td>\n",
              "      <td>10496</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Kernel Shape     Output Shape  Params  Mult-Adds\n",
              "Layer                                                            \n",
              "0_lstm                      -    [325370, 256]  277504     275456\n",
              "1_classification    [256, 41]  [1709, 256, 41]   10537      10496"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self): # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        # self.embedding = \n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=13,hidden_size= 256, num_layers= 1)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "        self.classification = nn.Linear(256,num_classes)# TODO: Create a single classification layer using nn.Linear()\n",
        "\n",
        "    def forward(self, x, lx): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
        "\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        packed_input = pack_padded_sequence(x,lx,enforce_sorted=False)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
        "\n",
        "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        out, lengths  = pad_packed_sequence(out1)# TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "\n",
        "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
        "        # out = # Optional: Do log softmax on the output. Which dimension?\n",
        "        # print(out[0,0,:])\n",
        "        out = torch.nn.functional.log_softmax(out,dim=2)\n",
        "        # print(out[0,0,:])\n",
        "        # print(sum(out[0,0,:]))\n",
        "        return out, lengths # TODO: Need to return 2 variables\n",
        "\n",
        "model = Network().to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx) # x and lx are from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Configuration (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this function calculates the Levenshtein distance \n",
        "\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
        "\n",
        "    # h - ouput from the model. Probability distributions at each time step \n",
        "    # y - target output sequence - sequence of Long tensors\n",
        "    # lh, ly - Lengths of output and target\n",
        "    # decoder - decoder object which was initialized in the previous cell\n",
        "    # PHONEME_MAP - maps output to a character to find the Levenshtein distance\n",
        "\n",
        "    h = h.permute(1, 0, 2)# TODO: You may need to transpose or permute h based on how you passed it to the criterion\n",
        "    # Print out the shapes often to debug\n",
        "    t1 =time.time()\n",
        "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
        "    t2 = time.time()\n",
        "    # print(f\"time cost {t2-t1}\")\n",
        "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
        "    # Input to the decode method will be h and its lengths lh \n",
        "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
        "\n",
        "    batch_size = h.shape[0]# TODO\n",
        "    # print(f\"batch_szie {batch_size}\")\n",
        "\n",
        "    dist = 0\n",
        "\n",
        "    # dist = 0\n",
        "    # h = np.zeros((100,))  \n",
        "    # y = y.cpu().detach().numpy().astype(int)\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "    # for j in range(100)\n",
        "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
        "    # print(h_sliced.shape)\n",
        "        # TODO: Get the output as a sequence of numbers from beam_results\n",
        "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
        "        # Same goes for beam_results and out_lens\n",
        "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
        "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
        "\n",
        "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        # print(f\"ly.shape {ly.shape}\")\n",
        "        # print(f\"y.shape {y.shape}\")\n",
        "        y_sliced = y[i][:ly[i]]\n",
        "        y_string = \"\".join([PHONEME_MAP[j] for j in y_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        # print(f\"h_string {h_string} h_string.len {len(h_string)}\")\n",
        "        # print(f\"y_string {y_string} y_string.len {len(y_string)}\")\n",
        "        per_dist = Levenshtein.distance(h_string, y_string)\n",
        "        # print(f\"{i} {per_dist} \")\n",
        "        dist += per_dist\n",
        "\n",
        "    dist/=batch_size\n",
        "    return dist\n",
        "    # print(f\"dist {dist}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  256\n",
            "Train dataset samples = 28539, batches = 112\n",
            "Val dataset samples = 2703, batches = 11\n",
            "Test dataset samples = 2620, batches = 11\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "lr = 2e-3\n",
        "batch_size = 256\n",
        "epochs = 40\n",
        "num_classes = 41\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": epochs,\n",
        "  \"batch_size\": batch_size\n",
        "}\n",
        "\n",
        "root = 'hw3p2_student_data/hw3p2_student_data' # TODO: Where your hw3p2_student_data folder is\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=train_data.collate_fn, num_workers=8)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False,collate_fn=val_data.collate_fn, num_workers=8)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=test_data.collate_fn, num_workers=8)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
        "\n",
        "criterion = nn.CTCLoss()# TODO: What loss do you need for sequence to sequence models? \n",
        "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)# TODO: Adam works well with LSTM (use lr = 2e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "\n",
        "decoder = ctcdecode.CTCBeamDecoder(labels = PHONEME_MAP,\n",
        "    model_path=None,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=40,\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=3,\n",
        "    num_processes=8,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True)# TODO: Intialize the CTC beam decoder\n",
        "# Check out https://github.com/parlance/ctcdecode for the details on how to implement decoding\n",
        "# Do you need to give log_probs_input = True or False?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3776684/2145559097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# num_correct = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbatch_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Train epoch: {epoch+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/torch1/lib/python3.7/site-packages/tqdm/__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m          \u001b[0;34m\"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m          TqdmDeprecationWarning, stacklevel=2)\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/torch1/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/torch1/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWARN_NOIPYW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
          ]
        }
      ],
      "source": [
        "# torch.cuda.empty_cache() # Use this often\n",
        "\n",
        "# TODO: Write the model evaluation function if you want to validate after every epoch\n",
        "\n",
        "# You are free to write your own code for model evaluation or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Note that when you give a higher beam width, decoding will take a longer time to get executed\n",
        "# Therefore, it is recommended that you calculate only the val dataset's Levenshtein distance (train not recommended) with a small beam width\n",
        "# When you are evaluating on your test set, you may have a higher beam width\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"7_1.0379955768585205_25_03_2022_22_30_06model.pth\"))\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # num_correct = 0\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc=f'Train epoch: {epoch+1}') \n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    desc = \"start\"\n",
        "    # tq = tqdm(train_loader, desc=desc,dynamic_ncols=True)\n",
        "    # with tqdm(train_loader, desc=desc,dynamic_ncols=True) as tq:\n",
        "    for i, (x, y, lx, ly) in enumerate(train_loader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        out, out_len = model(x,lx)\n",
        "        loss = criterion(out,y.permute(1,0), lx, ly)\n",
        "        total_loss += loss\n",
        "        # desc = \"loss = {:.04f}\".format(float(total_loss / (i + 1)))\n",
        "        # desc += \"lr = {:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # tq.update()\n",
        "        batch_bar.update() \n",
        "    batch_bar.close()\n",
        "    now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    torch.save(model.state_dict(), f\"{epoch}_{float(total_loss / len(train_loader))}_{now_name}model.pth\")\n",
        "    tqdm.write(\"Epoch {}/{}: Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "    wandb.log({\"Train Loss\": float(total_loss / len(train_loader))})\n",
        "    wandb.log({\"lr\" : float(optimizer.param_groups[0]['lr'])})\n",
        "\n",
        "    if epoch%5==0:\n",
        "        model.eval()\n",
        "        t_dist = 0\n",
        "        t_val_loss = 0\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            spectrograms, labels, input_lengths, label_lengths = data\n",
        "            # print(f\"label_lengths {label_lengths}\")\n",
        "            spectrograms, labels =spectrograms.to(device), labels\n",
        "            with torch.no_grad():\n",
        "                out,out_lengths = model(spectrograms,input_lengths)\n",
        "            t_val_loss += criterion(out,labels.permute(1,0), input_lengths, label_lengths)\n",
        "            t_dist += calculate_levenshtein(out, labels.permute(1,0), out_lengths, label_lengths, decoder, PHONEME_MAP)\n",
        "        dist = t_dist/len(val_loader)\n",
        "        val_loss = t_val_loss/len(val_loader)\n",
        "        tqdm.write(\"Epoch {}/{}: val loss {:.04f},distance {:.04f}\".format(\n",
        "            epoch + 1,\n",
        "            epochs,\n",
        "            float(val_loss),\n",
        "            float(dist)))\n",
        "        wandb.log({\"dist\" : float(dist)})\n",
        "        wandb.log({\"val_loss\" : float(val_loss)})\n",
        "            # break\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional but recommended\n",
        "# model.to(device)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(train_loader, 0):\n",
        "    spectrograms, labels, input_lengths, label_lengths = data\n",
        "    # print(f\"label_lengths {label_lengths}\")\n",
        "    spectrograms, labels =spectrograms.to(device), labels\n",
        "    out,out_lengths = model(spectrograms,input_lengths)\n",
        "    # print(f\"out_lengths {out_lengths}\")\n",
        "    # print(out.shape)\n",
        "    # oss = criterion(out,labels.permute(1,0), input_lengths, label_lengths)\n",
        "    # out = model(spectrograms,input_lengths)\n",
        "    # print(out.shape)\n",
        "    loss = criterion(out,labels.permute(1,0), out_lengths, label_lengths)\n",
        "    # print(labels)\n",
        "    calculate_levenshtein(out, labels.permute(1,0), out_lengths, label_lengths, decoder, PHONEME_MAP)\n",
        "    # Write a test code do perform a single forward pass and also compute the Levenshtein distance\n",
        "    # Make sure that you are able to get this right before going on to the actual training\n",
        "    # You may encounter a lot of shape errors\n",
        "    # Printing out the shapes will help in debugging\n",
        "    # Keep in mind that the Loss which you will use requires the input to be in a different format and the decoder expects it in a different format\n",
        "    # Make sure to read the corresponding docs about it\n",
        "    # pass\n",
        "\n",
        "    break # one iteration is enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\"6_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# TODO: Write the model training code \n",
        "\n",
        "\n",
        "# You are free to write your own code for training or you can use the code from previous homeworks' starter notebooks\n",
        "# However, you will have to make modifications because of the following.\n",
        "# (1) The dataloader returns 4 items unlike 2 for hw2p2\n",
        "# (2) The model forward returns 2 outputs\n",
        "# (3) The loss may require transpose or permuting\n",
        "\n",
        "# Tip: Implement mixed precision training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrqXnNqzJSc"
      },
      "source": [
        "# Submit to kaggle (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pred(h,lh,decoder,PHONEME_MAP):\n",
        "    h = h.permute(1, 0, 2)\n",
        "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
        "    h_string_lst = []\n",
        "    batch_size = h.shape[0]\n",
        "    for i in range(batch_size): \n",
        "\n",
        "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
        "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])\n",
        "        h_string_lst.append(h_string)\n",
        "    return h_string_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your model evaluation code for the test dataset\n",
        "# You can write your own code or use from the previous homewoks' stater notebooks\n",
        "# You can't calculate loss here. Why?\n",
        "def submit_test(model):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(1):\n",
        "            # X = test_samples[i]\n",
        "\n",
        "            # test_items = test_item(X, context=args['context'])\n",
        "            # test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for x, lx in test_loader:\n",
        "                # data = data.float().to(device)\n",
        "                x = x.cuda()\n",
        "                # y = y.cuda()\n",
        "                out, out_len = model(x,lx)\n",
        "                pred_y = pred(out,out_len,decoder,PHONEME_MAP)\n",
        "                # print(out.shape)\n",
        "                # pred_y = torch.argmax(out, axis=2)\n",
        "                # print(pred_y.shape)\n",
        "                pred_y_list.extend(pred_y)\n",
        "    # print(pred_y_list)\n",
        "    # now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    f = open(f\"bad.csv\", \"w\")\n",
        "    f.write(\"id,predictions\\n\")\n",
        "    for idx, i  in enumerate(pred_y_list):\n",
        "        f.write(f\"{idx},{i}\\n\")\n",
        "    f.close()\n",
        "\n",
        " \n",
        "    # with open('good.csv', 'w', newline='') as csvfile:\n",
        "    #     writer = csv.DictWriter(csvfile, fieldnames = ['id','label'])\n",
        "    #     # writer.writerow(['id','label'])\n",
        "    #     writer.writeheader() \n",
        "    #     for idx, i  in enumerate(pred_y_list):\n",
        "    #         writer.writerow([idx,i])\n",
        "        \n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the csv file\n",
        "submit_test(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100%|█████████████████████████████████████████| 185k/185k [00:00<00:00, 196kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "! kaggle competitions submit -c 11-785-s22-hw3p2 -f bad.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HW3P2_Starter.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
