{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Network(\n",
      "  (lstm): LSTM(13, 256)\n",
      "  (classification): Linear(in_features=256, out_features=41, bias=True)\n",
      ")\n",
      "Batch size:  256\n",
      "Train dataset samples = 28539, batches = 112\n",
      "Val dataset samples = 2703, batches = 11\n",
      "Test dataset samples = 2620, batches = 11\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/linjiw/.kaggle/kaggle.json'\n",
      "100%|█████████████████████████████████████████| 202k/202k [00:00<00:00, 238kB/s]\n",
      "Successfully submitted to Automatic Speech Recognition (ASR)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import phonemes\n",
    "# imports for decoding and distance calculation\n",
    "import ctcdecode\n",
    "import Levenshtein\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "\n",
    "# PHONEME_MAP is the list that maps the phoneme to a single character. \n",
    "# The dataset contains a list of phonemes but you need to map them to their corresponding characters to calculate the Levenshtein Distance\n",
    "# You final submission should not have the phonemes but the mapped string\n",
    "# No TODOs in this cell\n",
    "num_classes = 41\n",
    "PHONEME_MAP = [\n",
    "    \" \",\n",
    "    \".\", #SIL\n",
    "    \"a\", #AA\n",
    "    \"A\", #AE\n",
    "    \"h\", #AH\n",
    "    \"o\", #AO\n",
    "    \"w\", #AW\n",
    "    \"y\", #AY\n",
    "    \"b\", #B\n",
    "    \"c\", #CH\n",
    "    \"d\", #D\n",
    "    \"D\", #DH\n",
    "    \"e\", #EH\n",
    "    \"r\", #ER\n",
    "    \"E\", #EY\n",
    "    \"f\", #F\n",
    "    \"g\", #G\n",
    "    \"H\", #H\n",
    "    \"i\", #IH \n",
    "    \"I\", #IY\n",
    "    \"j\", #JH\n",
    "    \"k\", #K\n",
    "    \"l\", #L\n",
    "    \"m\", #M\n",
    "    \"n\", #N\n",
    "    \"N\", #NG\n",
    "    \"O\", #OW\n",
    "    \"Y\", #OY\n",
    "    \"p\", #P \n",
    "    \"R\", #R\n",
    "    \"s\", #S\n",
    "    \"S\", #SH\n",
    "    \"t\", #T\n",
    "    \"T\", #TH\n",
    "    \"u\", #UH\n",
    "    \"U\", #UW\n",
    "    \"v\", #V\n",
    "    \"W\", #W\n",
    "    \"?\", #Y\n",
    "    \"z\", #Z\n",
    "    \"Z\" #ZH\n",
    "]\n",
    "phe_dict = {}\n",
    "tensor_dict = {}\n",
    "PHONEMES = phonemes.PHONEMES\n",
    "for idx, i in enumerate(PHONEMES):\n",
    "    phe_dict[i] = PHONEME_MAP[idx]\n",
    "    tensor_dict[PHONEME_MAP[idx]] = idx\n",
    "\n",
    "def maplst(lst):\n",
    "    res =[]\n",
    "    for i in lst:\n",
    "        res.append(phe_dict[i])\n",
    "    res = np.array(res)\n",
    "    # res = res.astype(np.float)\n",
    "    return np.array(res)\n",
    "def maptotensor(lst):\n",
    "    res =[]\n",
    "    for i in lst:\n",
    "        res.append(tensor_dict[i])\n",
    "    res = np.array(res)\n",
    "    # res = res.astype(np.float)\n",
    "    return np.array(res)\n",
    "\n",
    "# This cell is where your actual TODOs start\n",
    "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
    "# The steps for implementation given below are how we have implemented it.\n",
    "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
    "\n",
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_path, partition= \"train\"): # You can use partition to specify train or dev\n",
    "\n",
    "        self.X_dir = os.path.join(data_path,partition,\"mfcc/\")# TODO: get mfcc directory path\n",
    "        self.Y_dir = os.path.join(data_path,partition,\"transcript/\")# TODO: get transcript path\n",
    "\n",
    "        self.X_files = os.listdir(self.X_dir)# TODO: list files in the mfcc directory\n",
    "        self.Y_files = os.listdir(self.Y_dir)# TODO: list files in the transcript directory\n",
    "\n",
    "        # TODO: store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
    "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
    "        self.PHONEMES = phonemes.PHONEMES\n",
    "\n",
    "        assert(len(self.X_files) == len(self.Y_files))\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_files)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        X_path = self.X_dir + self.X_files[ind]\n",
    "        Y_path = self.Y_dir + self.Y_files[ind]\n",
    "        X = torch.Tensor(np.load(X_path))# TODO: Load the mfcc npy file at the specified index ind in the directory\n",
    "        # Y = maplst(np.load(Y_path)[1:-1])# TODO: Load the corresponding transcripts\n",
    "        Y = np.load(Y_path)[1:-1]\n",
    "        # print(Y)\n",
    "        # print(Y.shape)\n",
    "        # Y2 = PHONEMES.index(i) for i in np.load(Y_path)[1:-1]\n",
    "        # print(f\"Y {Y}\")\n",
    "        # print(f\"Y2 {Y2}\")\n",
    "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
    "        # You need to convert these into a sequence of Long tensors\n",
    "        # Tip: You may need to use self.PHONEMES\n",
    "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them. \n",
    "        # You need to remove '<sos>' and '<eos>' from the trancripts. \n",
    "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
    "        Yy = torch.LongTensor([PHONEMES.index(i) for i in Y])\n",
    "        # Yy = torch.Tensor(maptotensor(Y)).type(torch.LongTensor)# TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
    "        # print(f\"X {X.shape}\")\n",
    "        # print(f\"Y {Y.shape}\")\n",
    "        return X, Yy\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "\n",
    "        batch_x = [x for x,y in batch]\n",
    "        batch_y = [y for x,y in batch]\n",
    "        # print(batch_x[0].shape)\n",
    "\n",
    "        new_lst = []\n",
    "        for idx, i in enumerate(batch_x):\n",
    "            new_lst.append(batch_x[idx])\n",
    "        batch_x_pad = pad_sequence(new_lst)\n",
    "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
    "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
    "        lengths_x_pad = [len(i) for i in batch_x_pad]\n",
    "        # print(f\"batch_x {batch_x}\")\n",
    "        \n",
    "\n",
    "        # print(f\"test_pad.len {test_pad.shape}\")\n",
    "        # print(f\"batch_x.len {len(batch_x)}\")\n",
    "        # print(f\"lengths_x {lengths_x}\")\n",
    "        # print(f\"lengths_x_pad {lengths_x_pad}\")\n",
    "\n",
    "        new_lst = []\n",
    "        for idx, i in enumerate(batch_y):\n",
    "            new_lst.append(batch_y[idx])\n",
    "        batch_y_pad = pad_sequence(new_lst)\n",
    "\n",
    "        # batch_y_pad = pad_sequence(batch_y) # TODO: pad the sequence with pad_sequence (already imported)\n",
    "        lengths_y = [len(i) for i in batch_y] # TODO: Get original lengths of the sequence before padding\n",
    "\n",
    "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "# You can either try to combine test data in the previous class or write a new Dataset class for test data\n",
    "class LibriSamplesTest(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_path, test_order): # test_order is the csv similar to what you used in hw1\n",
    "        self.data_path = data_path\n",
    "        test_csv_pth = os.path.join(data_path,'test',test_order)\n",
    "        subset = list(pd.read_csv(test_csv_pth).file)\n",
    "        # subset = self.parse_csv(test_csv_pth)\n",
    "        test_order_list = subset# TODO: open test_order.csv as a list\n",
    "        self.X_names = [i for i in subset]# TODO: Load the npy files from test_order.csv and append into a list\n",
    "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
    "    @staticmethod\n",
    "    def parse_csv(filepath):\n",
    "        subset = []\n",
    "        with open(filepath) as f:\n",
    "            f_csv = csv.reader(f)\n",
    "            for row in f_csv:\n",
    "                subset.append(row[0])\n",
    "        return subset[0:]\n",
    "    def __len__(self):\n",
    "        return len(self.X_names)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        # TODOs: Need to return only X because this is the test dataset\n",
    "        X_path = os.path.join(self.data_path,'test','mfcc',self.X_names[ind])\n",
    "        X = torch.Tensor(np.load(X_path))\n",
    "        return X\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batch_x = [x for x in batch]\n",
    "        new_lst = []\n",
    "        for idx, i in enumerate(batch_x):\n",
    "            new_lst.append(batch_x[idx])\n",
    "        batch_x_pad = pad_sequence(new_lst)\n",
    "        # batch_x_pad = pad_sequence([i for i in batch_x], batch_first=False)# TODO: pad the sequence with pad_sequence (already imported)\n",
    "        lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
    "        # lengths_x_pad = [len(i) for i in batch_x_pad]\n",
    "        # batch_x = [x for x in batch]\n",
    "        # batch_x_pad = pad_sequence(batch_x)# TODO: pad the sequence with pad_sequence (already imported)\n",
    "        # lengths_x = [len(i) for i in batch_x]# TODO: Get original lengths of the sequence before padding\n",
    "\n",
    "        return batch_x_pad, torch.tensor(lengths_x)\n",
    "\n",
    "# # Optional\n",
    "# # Test code for checking shapes and return arguments of the train and val loaders\n",
    "\n",
    "\n",
    "# for data in train_loader:\n",
    "#     x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
    "#     # print(f\"ly {ly}\")\n",
    "#     # print(f\"lx {lx}\")\n",
    "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "#     break\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self): # You can add any extra arguments as you wish\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better \n",
    "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
    "        # self.embedding = \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=13,hidden_size= 256, num_layers= 1)# TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
    "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "        self.classification = nn.Linear(256,num_classes)# TODO: Create a single classification layer using nn.Linear()\n",
    "\n",
    "    def forward(self, x, lx): # TODO: You need to pass atleast 1 more parameter apart from self and x\n",
    "\n",
    "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
    "        packed_input = pack_padded_sequence(x,lx,enforce_sorted=False)# TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires\n",
    "\n",
    "        out1, (out2, out3) = self.lstm(packed_input)# TODO: Pass packed input to self.lstm\n",
    "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
    "        out, lengths  = pad_packed_sequence(out1)# TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
    "\n",
    "        out = self.classification(out)# TODO: Pass unpacked LSTM output to the classification layer\n",
    "        # out = # Optional: Do log softmax on the output. Which dimension?\n",
    "        # print(out[0,0,:])\n",
    "        out = torch.nn.functional.log_softmax(out,dim=2)\n",
    "        # print(out[0,0,:])\n",
    "        # print(sum(out[0,0,:]))\n",
    "        return out, lengths # TODO: Need to return 2 variables\n",
    "\n",
    "model = Network().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def calculate_levenshtein(h, y, lh, ly, decoder, PHONEME_MAP):\n",
    "\n",
    "    # h - ouput from the model. Probability distributions at each time step \n",
    "    # y - target output sequence - sequence of Long tensors\n",
    "    # lh, ly - Lengths of output and target\n",
    "    # decoder - decoder object which was initialized in the previous cell\n",
    "    # PHONEME_MAP - maps output to a character to find the Levenshtein distance\n",
    "\n",
    "    h = h.permute(1, 0, 2)# TODO: You may need to transpose or permute h based on how you passed it to the criterion\n",
    "    # Print out the shapes often to debug\n",
    "    t1 =time.time()\n",
    "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
    "    t2 = time.time()\n",
    "    # print(f\"time cost {t2-t1}\")\n",
    "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
    "    # Input to the decode method will be h and its lengths lh \n",
    "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
    "\n",
    "    batch_size = h.shape[0]# TODO\n",
    "    # print(f\"batch_szie {batch_size}\")\n",
    "\n",
    "    dist = 0\n",
    "\n",
    "    # dist = 0\n",
    "    # h = np.zeros((100,))  \n",
    "    # y = y.cpu().detach().numpy().astype(int)\n",
    "    for i in range(batch_size): # Loop through each element in the batch\n",
    "\n",
    "    # for j in range(100)\n",
    "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
    "    # print(h_sliced.shape)\n",
    "        # TODO: Get the output as a sequence of numbers from beam_results\n",
    "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
    "        # Same goes for beam_results and out_lens\n",
    "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
    "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
    "\n",
    "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
    "        # print(f\"ly.shape {ly.shape}\")\n",
    "        # print(f\"y.shape {y.shape}\")\n",
    "        y_sliced = y[i][:ly[i]]\n",
    "        y_string = \"\".join([PHONEME_MAP[j] for j in y_sliced])# TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
    "        # print(f\"h_string {h_string} h_string.len {len(h_string)}\")\n",
    "        # print(f\"y_string {y_string} y_string.len {len(y_string)}\")\n",
    "        per_dist = Levenshtein.distance(h_string, y_string)\n",
    "        # print(f\"{i} {per_dist} \")\n",
    "        dist += per_dist\n",
    "\n",
    "    dist/=batch_size\n",
    "    return dist\n",
    "    # print(f\"dist {dist}\")\n",
    "\n",
    "\n",
    "\n",
    "lr = 2e-3\n",
    "batch_size = 256\n",
    "epochs = 40\n",
    "num_classes = 41\n",
    "# wandb.config = {\n",
    "#   \"learning_rate\": lr,\n",
    "#   \"epochs\": epochs,\n",
    "#   \"batch_size\": batch_size\n",
    "# }\n",
    "\n",
    "root = 'hw3p2_student_data/hw3p2_student_data' # TODO: Where your hw3p2_student_data folder is\n",
    "\n",
    "train_data = LibriSamples(root, 'train')\n",
    "val_data = LibriSamples(root, 'dev')\n",
    "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=train_data.collate_fn, num_workers=8)# TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "val_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False,collate_fn=val_data.collate_fn, num_workers=8)# TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=False,collate_fn=test_data.collate_fn, num_workers=8)# TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument \n",
    "\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
    "\n",
    "criterion = nn.CTCLoss()# TODO: What loss do you need for sequence to sequence models? \n",
    "# Do you need to transpose or permute the model output to find out the loss? Read its documentation\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)# TODO: Adam works well with LSTM (use lr = 2e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
    "\n",
    "decoder = ctcdecode.CTCBeamDecoder(labels = PHONEME_MAP,\n",
    "    model_path=None,\n",
    "    alpha=0,\n",
    "    beta=0,\n",
    "    cutoff_top_n=40,\n",
    "    cutoff_prob=1.0,\n",
    "    beam_width=3,\n",
    "    num_processes=8,\n",
    "    blank_id=0,\n",
    "    log_probs_input=True)# TODO: Intialize the CTC beam decoder\n",
    "# Check out https://github.com/parlance/ctcdecode for the details on how to implement decoding\n",
    "# Do you need to give log_probs_input = True or False?\n",
    "\n",
    "def pred(h,lh,decoder,PHONEME_MAP):\n",
    "    h = h.permute(1, 0, 2)\n",
    "    beam_results, _, _, out_lens = decoder.decode(h,seq_lens=lh)\n",
    "    h_string_lst = []\n",
    "    batch_size = h.shape[0]\n",
    "    for i in range(batch_size): \n",
    "\n",
    "        h_sliced = beam_results[i][0][:out_lens[i,0]]\n",
    "        h_string = \"\".join([PHONEME_MAP[j] for j in h_sliced])\n",
    "        h_string_lst.append(h_string)\n",
    "    return h_string_lst\n",
    "\n",
    "# TODO: Write your model evaluation code for the test dataset\n",
    "# You can write your own code or use from the previous homewoks' stater notebooks\n",
    "# You can't calculate loss here. Why?\n",
    "def submit_test(model):\n",
    "    model.eval()\n",
    "    true_y_list = []\n",
    "    pred_y_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1):\n",
    "            # X = test_samples[i]\n",
    "\n",
    "            # test_items = test_item(X, context=args['context'])\n",
    "            # test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "            for x, lx in test_loader:\n",
    "                # data = data.float().to(device)\n",
    "                x = x.cuda()\n",
    "                # y = y.cuda()\n",
    "                out, out_len = model(x,lx)\n",
    "                pred_y = pred(out,out_len,decoder,PHONEME_MAP)\n",
    "                # print(out.shape)\n",
    "                # pred_y = torch.argmax(out, axis=2)\n",
    "                # print(pred_y.shape)\n",
    "                pred_y_list.extend(pred_y)\n",
    "    # print(pred_y_list)\n",
    "    # now_name = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    f = open(f\"bad.csv\", \"w\")\n",
    "    f.write(\"id,predictions\\n\")\n",
    "    for idx, i  in enumerate(pred_y_list):\n",
    "        f.write(f\"{idx},{i}\\n\")\n",
    "    f.close()\n",
    "\n",
    "model.load_state_dict(torch.load(\"99_0.6917038559913635_26_03_2022_03_17_20model.pth\"))\n",
    "\n",
    "submit_test(model)\n",
    "\n",
    "! kaggle competitions submit -c 11-785-s22-hw3p2 -f bad.csv -m \"Message\"\n",
    "\n",
    "    # with open('good.csv', 'w', newline='') as csvfile:\n",
    "    #     writer = csv.DictWriter(csvfile, fieldnames = ['id','label'])\n",
    "    #     # writer.writerow(['id','label'])\n",
    "    #     writer.writeheader() \n",
    "    #     for idx, i  in enumerate(pred_y_list):\n",
    "    #         writer.writerow([idx,i])\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39bdd053050b0b7cc4101130ec1092e4b57ec87bfe48efa8a1b99dab6f674493"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('torch1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
